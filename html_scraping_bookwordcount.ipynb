{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38da145",
   "metadata": {},
   "source": [
    "## html_scraping_bookwordcount\n",
    "\n",
    "Script to extract words from a html book, find the most commonly occuring words and save them to a csv file.\n",
    "\n",
    "Note: \"stop words\" are excluded, currently set to English.\n",
    "\n",
    "Recommended source of books is from the Guttenburg Project, where the html versions should be chosen.\n",
    "<br>Most popular books: https://gutenberg.org/browse/scores/top\n",
    "<br>Books in Spanish: https://gutenberg.org/browse/languages/es\n",
    "\n",
    "Code modified from DataCamp.com 'Word Frequency' project.\n",
    "\n",
    "Some packages will likely require installing for the script to run.\n",
    "\n",
    "Merlin Fair\n",
    "<br>Created: 2024/02/27\n",
    "<br>Last Modified: 2024/02/27 (MF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c02a9dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mjf81/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and download packages\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "975173a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Moby Dick HTML  \n",
    "r = requests.get('https://www.gutenberg.org/cache/epub/2701/pg2701-images.html')\n",
    "\n",
    "# Set the correct text encoding of the HTML page\n",
    "r.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e712c0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0086\" class=\"pginternal\"> CHAPTER 86. The Tail. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0087\" class=\"pginternal\"> CHAPTER 87. The Grand Armada. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0088\" class=\"pginternal\"> CHAPTER 88. Schools and Schoolmasters. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0089\" class=\"pginternal\"> CHAPTER 89. Fast-Fish and Loose-Fish. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0090\" class=\"pginternal\"> CHAPTER 90. Heads or Tails. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0091\" class=\"pginternal\"> CHAPTER 91. The Pequod Meets The Rose-Bud. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0092\" class=\"pginternal\"> CHAPTER 92. Ambergris. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0093\" class=\"pginternal\"> CHAPTER 93. The Castaway. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0094\" class=\"pginternal\"> CHAPTER 94. A Squeeze of the Hand. </a>\r\n",
      "</p>\r\n",
      "<p class=\"toc\">\r\n",
      "<a href=\"#link2HCH0095\" cl\n"
     ]
    }
   ],
   "source": [
    "# Extract the HTML from the request object\n",
    "html = r.text\n",
    "\n",
    "# Print 1000 characters in html\n",
    "print(html[15000:16000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f6ff9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BeautifulSoup object from the HTML\n",
    "html_soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Get the text out of the soup\n",
    "moby_text = html_soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "71736727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(moby_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ea3ffd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list called words containing all tokens transformed to lowercase\n",
    "words = [token.lower() for token in tokens]\n",
    "\n",
    "# Print out the first eight words\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c6ce646f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the English stop words from nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Print out the first eight stop words\n",
    "stop_words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1984ee32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project', 'gutenberg', 'ebook', 'moby', 'dick']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list words_ns containing all words that are in words but not in stop_words\n",
    "words_no_stop = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Print the first five words_no_stop to check that stop words are gone\n",
    "words_no_stop[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4dd018a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whale', 1244), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Counter object from our processed list of words\n",
    "count_total = Counter(words_no_stop)\n",
    "\n",
    "# Store ten most common words and their counts as top_ten\n",
    "top_ten = count_total.most_common(10)\n",
    "\n",
    "# Print the top ten words and their counts\n",
    "print(top_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0b0eab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving top 100 to CSV file\n",
    "# N.b. change filename to reflect chosen book\n",
    "filename = 'words_mobydick.csv'\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "\n",
    "    writer = csv.writer(csvfile, delimiter=',',  quotechar='\"', \n",
    "                                     quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow([\"word\",\"count\"])\n",
    "    for key, count in count_total.most_common(100):\n",
    "        word = key\n",
    "        writer.writerow([word, count])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
